{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dfbefdf2-399f-4bf8-ab1d-7e1a7b6a3a34",
      "metadata": {
        "id": "dfbefdf2-399f-4bf8-ab1d-7e1a7b6a3a34"
      },
      "source": [
        "## Reproduce the results using the models trained and stored in the models folder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "55ae034c-6354-4589-8ed6-dcb94b4b7518",
      "metadata": {
        "id": "55ae034c-6354-4589-8ed6-dcb94b4b7518"
      },
      "outputs": [],
      "source": [
        "from Utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dd27b8b-97a0-4130-8047-b359dd24d7b5",
      "metadata": {
        "id": "3dd27b8b-97a0-4130-8047-b359dd24d7b5"
      },
      "outputs": [],
      "source": [
        "models_path='$HOME/Datasets/QuoraQuestionPairs/models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "547bbb76-58ee-4b90-bf0a-03a9104f9064",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "547bbb76-58ee-4b90-bf0a-03a9104f9064",
        "outputId": "2150e383-bdbf-46ba-e7d0-fbbea22aace4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_df.shape= (15363, 6)\n",
            "test_df.shape= (16172, 6)\n"
          ]
        }
      ],
      "source": [
        "# Create validation and test partitions\n",
        "#quora_df = pd.read_csv(\"$HOME/Datasets/QuoraQuestionPairs/quora_data.csv\")\n",
        "quora_df = pd.read_csv(\"/Users/user/Documents/GitHub/nlp_deliv_1/quora_data.csv\")\n",
        "A_df, test_df = sklearn.model_selection.train_test_split(quora_df, test_size=0.05, random_state=123)\n",
        "train_df, val_df = sklearn.model_selection.train_test_split(A_df, test_size=0.05)\n",
        "print('val_df.shape=',val_df.shape)\n",
        "print('test_df.shape=',test_df.shape)\n",
        "\n",
        "y_val = val_df[\"is_duplicate\"].values\n",
        "y_test = test_df[\"is_duplicate\"].values\n",
        "\n",
        "# cast to list taking care of nans:\n",
        "q1_val =  cast_list_as_strings(list(val_df[\"question1\"]))\n",
        "q2_val =  cast_list_as_strings(list(val_df[\"question2\"]))\n",
        "q1_test  =  cast_list_as_strings(list(test_df[\"question1\"]))\n",
        "q2_test  =  cast_list_as_strings(list(test_df[\"question2\"]))\n",
        "q1_train =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
        "q2_train =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
        "\n",
        "all_questions = q1_train + q2_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d8399e",
      "metadata": {},
      "source": [
        "## 1. Load the teachers baseline model and print accuracy:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b155512c",
      "metadata": {},
      "source": [
        "Load Teachers baseline ad the vectoirizer used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ae7e39",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from models/teacher_baseline.pkl\n",
            "Model loaded from models/count_vectorizer.pkl\n"
          ]
        }
      ],
      "source": [
        "logistic = load_model(path=models_path, filename=\"teacher_baseline.pkl\")\n",
        "count_vectorizer = load_model(path=models_path, filename=\"count_vectorizer.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bd445eb",
      "metadata": {},
      "source": [
        "And now evaluate on the VALIDATION set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec37c71e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with count vectoriser: 0.8082\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#get the validation feature for the count vectorizer:\n",
        "X_val_q1q2 = get_features_from_df(val_df, count_vectorizer)\n",
        "\n",
        "#accuracy of the regr with count vectorizer\n",
        "y_pred = logistic.predict(X_val_q1q2)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy with count vectoriser: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c2a0060",
      "metadata": {},
      "source": [
        "## 2. Using custom TFIDF_Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd5dbe9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from models/custom_tfidf_logreg.pkl\n",
            "Model loaded from models/custom_tfidf_vectorizer.pkl\n"
          ]
        }
      ],
      "source": [
        "#load model and vectorizer:\n",
        "logistic2 = load_model(models_path, \"custom_tfidf_logreg.pkl\")\n",
        "custom_vectorizer = load_model(models_path, \"custom_tfidf_vectorizer.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0227437a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy new approach with custom TFIDF vectorizer: 0.8563\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_val_q1q2_tfidf = get_features_from_df_tfidf(val_df, custom_vectorizer)\n",
        "\n",
        "y_pred = logistic2.predict(X_val_q1q2_tfidf)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy new approach with custom TFIDF vectorizer: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f55f41",
      "metadata": {},
      "source": [
        "## 3. Using built-in TFIDF vectorizer on charecter level with 4-grams, and a cosinus similarity between the pairs metric feature "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c4486d1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from models/tfidf_4_char_gram_logreg.pkl\n",
            "Model loaded from models/tfidf_vectorizer.pkl\n"
          ]
        }
      ],
      "source": [
        "logistic4 = load_model(models_path, \"tfidf_4_char_gram_logreg.pkl\")\n",
        "tfidf_vectorizer = load_model(models_path, \"tfidf_vectorizer.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3d21585",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy new approach with custom TFIDF vectorizer: 0.8325\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from Utils import *\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import sparse\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "val_df['char_ngram_sim'] = val_df.apply(lambda row: char_ngram_similarity(row['question1'], row['question2'], tfidf_vectorizer), axis=1)\n",
        "char_sim_sparse = sparse.csr_matrix(val_df['char_ngram_sim'].values).T  # make it column vector\n",
        "\n",
        "X_val_q1q2_bi_tfidf = get_features_from_df_tfidf(val_df, tfidf_vectorizer)\n",
        "\n",
        "X_train_combined = hstack([X_val_q1q2_bi_tfidf, char_sim_sparse])\n",
        "\n",
        "y_pred = logistic4.predict(X_train_combined)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy new approach with TFIDF vectorizer and 4-gram cosin-similarity: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1de4e1e",
      "metadata": {},
      "source": [
        "## 4. Using log regression with built in TFIDF, 4-gram character cosin similarity feature and feature whether it question starts with specific question word "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f36266c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from models/tfidf_5_char_gram_word_starts_logreg.pkl\n",
            "Model loaded from models/scaler_tfidf_5_char_gram_word_starts_logreg.pkl\n",
            "Model loaded from models/tfidf_vectorizer.pkl\n"
          ]
        }
      ],
      "source": [
        "logistic5 = load_model(models_path, \"tfidf_5_char_gram_word_starts_logreg.pkl\")\n",
        "scaler = load_model(models_path, \"scaler_tfidf_5_char_gram_word_starts_logreg.pkl\")\n",
        "tfidf_vectorizer = load_model(models_path, \"tfidf_vectorizer.pkl\")\n",
        "import pickle\n",
        "# We need them in the same order as in training!!!\n",
        "with open(models_path + '/feature_cols_train_start_words.pkl', 'rb') as f:\n",
        "    feature_cols_train = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be3e050d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['q1_starts_with_how', 'q1_starts_with_can', 'q1_starts_with_what', 'q1_starts_with_why', 'q1_starts_with_are', 'q1_starts_with_do', 'q1_starts_with_does', 'q1_starts_with_is', 'q1_starts_with_should', 'q1_starts_with_could', 'q2_starts_with_how', 'q2_starts_with_can', 'q2_starts_with_what', 'q2_starts_with_why', 'q2_starts_with_are', 'q2_starts_with_do', 'q2_starts_with_does', 'q2_starts_with_is', 'q2_starts_with_should', 'q2_starts_with_could']\n",
            "        q1_starts_with_how  q1_starts_with_can  q1_starts_with_what  \\\n",
            "53593                    0                   0                    0   \n",
            "100923                   0                   0                    1   \n",
            "226707                   1                   0                    0   \n",
            "25127                    0                   1                    0   \n",
            "15382                    0                   0                    0   \n",
            "...                    ...                 ...                  ...   \n",
            "249858                   0                   0                    0   \n",
            "9936                     0                   0                    1   \n",
            "166841                   1                   0                    0   \n",
            "202575                   0                   0                    1   \n",
            "75037                    0                   0                    0   \n",
            "\n",
            "        q1_starts_with_why  q1_starts_with_are  q1_starts_with_do  \\\n",
            "53593                    0                   0                  0   \n",
            "100923                   0                   0                  0   \n",
            "226707                   0                   0                  0   \n",
            "25127                    0                   0                  0   \n",
            "15382                    0                   0                  0   \n",
            "...                    ...                 ...                ...   \n",
            "249858                   0                   0                  0   \n",
            "9936                     0                   0                  0   \n",
            "166841                   0                   0                  0   \n",
            "202575                   0                   0                  0   \n",
            "75037                    1                   0                  0   \n",
            "\n",
            "        q1_starts_with_does  q1_starts_with_is  q1_starts_with_should  \\\n",
            "53593                     0                  0                      0   \n",
            "100923                    0                  0                      0   \n",
            "226707                    0                  0                      0   \n",
            "25127                     0                  0                      0   \n",
            "15382                     0                  0                      0   \n",
            "...                     ...                ...                    ...   \n",
            "249858                    0                  0                      0   \n",
            "9936                      0                  0                      0   \n",
            "166841                    0                  0                      0   \n",
            "202575                    0                  0                      0   \n",
            "75037                     0                  0                      0   \n",
            "\n",
            "        q1_starts_with_could  q2_starts_with_how  q2_starts_with_can  \\\n",
            "53593                      0                   0                   0   \n",
            "100923                     0                   0                   0   \n",
            "226707                     0                   1                   0   \n",
            "25127                      0                   0                   0   \n",
            "15382                      0                   1                   0   \n",
            "...                      ...                 ...                 ...   \n",
            "249858                     0                   0                   0   \n",
            "9936                       0                   0                   0   \n",
            "166841                     0                   0                   0   \n",
            "202575                     0                   0                   0   \n",
            "75037                      0                   0                   0   \n",
            "\n",
            "        q2_starts_with_what  q2_starts_with_why  q2_starts_with_are  \\\n",
            "53593                     1                   0                   0   \n",
            "100923                    1                   0                   0   \n",
            "226707                    0                   0                   0   \n",
            "25127                     0                   0                   0   \n",
            "15382                     0                   0                   0   \n",
            "...                     ...                 ...                 ...   \n",
            "249858                    1                   0                   0   \n",
            "9936                      1                   0                   0   \n",
            "166841                    1                   0                   0   \n",
            "202575                    1                   0                   0   \n",
            "75037                     1                   0                   0   \n",
            "\n",
            "        q2_starts_with_do  q2_starts_with_does  q2_starts_with_is  \\\n",
            "53593                   0                    0                  0   \n",
            "100923                  0                    0                  0   \n",
            "226707                  0                    0                  0   \n",
            "25127                   0                    0                  0   \n",
            "15382                   0                    0                  0   \n",
            "...                   ...                  ...                ...   \n",
            "249858                  0                    0                  0   \n",
            "9936                    0                    0                  0   \n",
            "166841                  0                    0                  0   \n",
            "202575                  0                    0                  0   \n",
            "75037                   0                    0                  0   \n",
            "\n",
            "        q2_starts_with_should  q2_starts_with_could  \n",
            "53593                       0                     0  \n",
            "100923                      0                     0  \n",
            "226707                      0                     0  \n",
            "25127                       0                     0  \n",
            "15382                       0                     0  \n",
            "...                       ...                   ...  \n",
            "249858                      0                     0  \n",
            "9936                        0                     0  \n",
            "166841                      0                     0  \n",
            "202575                      0                     0  \n",
            "75037                       0                     0  \n",
            "\n",
            "[15363 rows x 20 columns]\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import hstack\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# start_words = ['how', 'can', 'what', 'why', 'are', 'do', 'does', 'is', 'should', 'could']\n",
        "\n",
        "start_feats_q1 = val_df['question1'].apply(lambda x: pd.Series(starts_with_indicator(x)))\n",
        "start_feats_q1.columns = [f'q1_{col}' for col in start_feats_q1.columns]\n",
        "\n",
        "start_feats_q2 = val_df['question2'].apply(lambda x: pd.Series(starts_with_indicator(x)))\n",
        "start_feats_q2.columns = [f'q2_{col}' for col in start_feats_q2.columns]\n",
        "\n",
        "val_df = pd.concat([val_df, start_feats_q1, start_feats_q2], axis=1)\n",
        "\n",
        "#print(feature_cols_train)\n",
        "val_numeric_features = val_df[feature_cols_train]\n",
        "val_numeric_features = val_numeric_features[feature_cols_train]\n",
        "#print(val_numeric_features)\n",
        "\n",
        "# Scale numeric features before combining with sparse ones\n",
        "\n",
        "numeric_scaled = scaler.transform(val_numeric_features)\n",
        "numeric_sparse = sparse.csr_matrix(numeric_scaled)\n",
        "\n",
        "val_df['char_ngram_sim'] = val_df.apply(lambda row: char_ngram_similarity(row['question1'], row['question2'], tfidf_vectorizer), axis=1)\n",
        "char_sim_sparse = sparse.csr_matrix(val_df['char_ngram_sim'].values).T  # make it column vector\n",
        "\n",
        "X_val_q1q2_bi_tfidf = get_features_from_df_tfidf(val_df, tfidf_vectorizer)\n",
        "\n",
        "# we're using the 4-gram \n",
        "X_train_combined = hstack([X_val_q1q2_bi_tfidf, char_sim_sparse, numeric_sparse])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c372e9d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy new approach with TFIDF vectorizer and 4-gram cosin-similarity and feature for start word: 0.6071\n"
          ]
        }
      ],
      "source": [
        "y_pred = logistic5.predict(X_train_combined)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy new approach with TFIDF vectorizer and 4-gram cosin-similarity and feature for start word: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b46c819f",
      "metadata": {},
      "source": [
        "## 5. Use Jackard similarity and TFIDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0142eddd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from models/tfidf_jackard_logreg.pkl\n",
            "Model loaded from models/tfidf_vectorizer.pkl\n"
          ]
        }
      ],
      "source": [
        "logistic6 = load_model(models_path, \"tfidf_jackard_logreg.pkl\")\n",
        "tfidf_vectorizer = load_model(models_path, \"tfidf_vectorizer.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "713ffd6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "val_df['jaccard_sim'] = val_df.apply(lambda row: jaccard_similarity(row['question1'], row['question2']), axis=1)\n",
        "\n",
        "X_val_q1q2_bi_tfidf = get_features_from_df_tfidf(val_df, tfidf_vectorizer)\n",
        "\n",
        "# we're using the 4-gram \n",
        "X_train_combined = hstack([X_val_q1q2_bi_tfidf, val_df['jaccard_sim'].values.reshape(-1, 1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ea8443e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy new approach with TFIDF vectorizer and 4-gram cosin-similarity and feature for start word: 0.8200\n"
          ]
        }
      ],
      "source": [
        "y_pred = logistic6.predict(X_train_combined)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy new approach with TFIDF vectorizer and Jackard similarity: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dca8fa6",
      "metadata": {},
      "source": [
        "## 6. Custom TFIDF and Jackard similarity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ff6dafc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from models/custom_tfidf_jackard_logreg.pkl\n",
            "Model loaded from models/custom_tfidf_vectorizer.pkl\n"
          ]
        }
      ],
      "source": [
        "logistic7 = load_model(models_path, \"custom_tfidf_jackard_logreg.pkl\")\n",
        "custom_vectorizer = load_model(models_path, \"custom_tfidf_vectorizer.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a20f7d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "val_df['jaccard_sim'] = val_df.apply(lambda row: jaccard_similarity(row['question1'], row['question2']), axis=1)\n",
        "\n",
        "X_val_q1q2_tfidf = get_features_from_df_tfidf(val_df, custom_vectorizer)\n",
        "\n",
        "# we're using the 4-gram \n",
        "X_train_combined = hstack([X_val_q1q2_tfidf, val_df['jaccard_sim'].values.reshape(-1, 1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcd1ddba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy new approach with custom TFIDF vectorizer and Jackard similarity: 0.8735\n"
          ]
        }
      ],
      "source": [
        "y_pred = logistic7.predict(X_train_combined)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy new approach with custom TFIDF vectorizer and Jackard similarity: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18634027",
      "metadata": {},
      "source": [
        "Load Teachers baseline ad the vectoirizer used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b20943",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from models/teacher_baseline.pkl\n",
            "Model loaded from models/count_vectorizer.pkl\n"
          ]
        }
      ],
      "source": [
        "logistic = load_model(path=models_path, filename=\"teacher_baseline.pkl\")\n",
        "count_vectorizer = load_model(path=models_path, filename=\"count_vectorizer.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace0673f",
      "metadata": {},
      "source": [
        "And now evaluate on the VALIDATION set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffec9b94",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with count vectoriser: 0.8082\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#get the validation feature for the count vectorizer:\n",
        "X_val_q1q2 = get_features_from_df(val_df, count_vectorizer)\n",
        "\n",
        "#accuracy of the regr with count vectorizer\n",
        "y_pred = logistic.predict(X_val_q1q2)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy with count vectoriser: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2b5a3fc",
      "metadata": {
        "id": "a2b5a3fc"
      },
      "source": [
        "# 7. TF-IDF + xgboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73eed175",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73eed175",
        "outputId": "90bf023a-b894-4eae-f31e-a424f584916a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from /content/model_tf_xgb.pkl\n",
            "Model loaded from /content/tfidf.pkl\n",
            "Accuracy for approach with TFIDF vectorizer and XGBoost: 0.7705\n"
          ]
        }
      ],
      "source": [
        "model_tf_xgb = load_model(models_path, \"model_tf_xgb.pkl\")\n",
        "\n",
        "q1_train = pd.Series(cast_list_as_strings(list(train_df[\"question1\"])))\n",
        "q2_train = pd.Series(cast_list_as_strings(list(train_df[\"question2\"])))\n",
        "q1_val = pd.Series(cast_list_as_strings(list(val_df[\"question1\"])))\n",
        "q2_val = pd.Series(cast_list_as_strings(list(val_df[\"question2\"])))\n",
        "q1_test = pd.Series(cast_list_as_strings(list(test_df[\"question1\"])))\n",
        "q2_test = pd.Series(cast_list_as_strings(list(test_df[\"question2\"])))\n",
        "# Prepare TF-IDF\n",
        "\n",
        "#tfidf = TfidfVectorizer(ngram_range=(1,1))\n",
        "\n",
        "tfidf = load_model(models_path, \"tfidf.pkl\")\n",
        "\n",
        "\n",
        "X_train_tfidf = create_tfidf_features(q1_train, q2_train, tfidf)\n",
        "X_val_tfidf   = create_tfidf_features(q1_val, q2_val, tfidf)\n",
        "X_test_tfidf  = create_tfidf_features(q1_test, q2_test, tfidf)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = model_tf_xgb.predict(X_val_tfidf)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy for approach with TFIDF vectorizer and XGBoost: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5559a06d",
      "metadata": {
        "id": "5559a06d"
      },
      "source": [
        "### 8. word2vec + logreg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "gWDP98I6A1v7",
      "metadata": {
        "id": "gWDP98I6A1v7"
      },
      "outputs": [],
      "source": [
        "q1_train = pd.Series(cast_list_as_strings(list(train_df[\"question1\"])))\n",
        "q2_train = pd.Series(cast_list_as_strings(list(train_df[\"question2\"])))\n",
        "q1_val = pd.Series(cast_list_as_strings(list(val_df[\"question1\"])))\n",
        "q2_val = pd.Series(cast_list_as_strings(list(val_df[\"question2\"])))\n",
        "q1_test = pd.Series(cast_list_as_strings(list(test_df[\"question1\"])))\n",
        "q2_test = pd.Series(cast_list_as_strings(list(test_df[\"question2\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "201dacac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "201dacac",
        "outputId": "1d2a64dc-a850-4c94-9150-3aaf433c81f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for approach with Word2Vec vectorizer and Logistic Regression: 0.7276\n"
          ]
        }
      ],
      "source": [
        "w2v_model = load_model(models_path, \"w2v_model.pkl\")\n",
        "model_w2v_log = load_model(models_path, \"model_w2v_log.pkl\")\n",
        "q1 =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
        "q2 =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
        "all_questions = q1 + q2\n",
        "\n",
        "# Tokenize for Word2Vec\n",
        "tokenized = [text.lower().split() for text in all_questions]\n",
        "\n",
        "# 3. Train Word2Vec\n",
        "w2v_size = 256\n",
        "# w2v_model = Word2Vec(sentences=tokenized, vector_size=w2v_size,\n",
        "#                      window=5, min_count=1, workers=4, seed=42)\n",
        "\n",
        "X_train_w2v = create_w2v_features(q1_train, q2_train, w2v_model, w2v_size)\n",
        "X_val_w2v   = create_w2v_features(q1_val, q2_val, w2v_model, w2v_size)\n",
        "X_test_w2v  = create_w2v_features(q1_test, q2_test, w2v_model, w2v_size)\n",
        "\n",
        "y_pred = model_w2v_log.predict(X_val_w2v)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy for approach with Word2Vec vectorizer and Logistic Regression: {accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
