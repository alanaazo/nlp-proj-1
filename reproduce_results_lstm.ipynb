{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbefdf2-399f-4bf8-ab1d-7e1a7b6a3a34",
   "metadata": {
    "id": "dfbefdf2-399f-4bf8-ab1d-7e1a7b6a3a34"
   },
   "source": [
    "## Reproduce the results using the models trained and stored in the models folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ae034c-6354-4589-8ed6-dcb94b4b7518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:38:33.490574Z",
     "iopub.status.busy": "2025-05-05T07:38:33.490216Z",
     "iopub.status.idle": "2025-05-05T07:38:44.285598Z",
     "shell.execute_reply": "2025-05-05T07:38:44.284783Z"
    },
    "id": "55ae034c-6354-4589-8ed6-dcb94b4b7518"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 09:38:40.578622: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from Utils_lstm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd27b8b-97a0-4130-8047-b359dd24d7b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T07:38:44.289792Z",
     "iopub.status.busy": "2025-05-05T07:38:44.289299Z",
     "iopub.status.idle": "2025-05-05T07:38:44.293301Z",
     "shell.execute_reply": "2025-05-05T07:38:44.292365Z"
    },
    "id": "3dd27b8b-97a0-4130-8047-b359dd24d7b5"
   },
   "outputs": [],
   "source": [
    "models_path='$HOME/Datasets/QuoraQuestionPairs/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547bbb76-58ee-4b90-bf0a-03a9104f9064",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-05T07:38:44.297072Z",
     "iopub.status.busy": "2025-05-05T07:38:44.296751Z",
     "iopub.status.idle": "2025-05-05T07:38:45.438961Z",
     "shell.execute_reply": "2025-05-05T07:38:45.438212Z"
    },
    "id": "547bbb76-58ee-4b90-bf0a-03a9104f9064",
    "outputId": "f13505ed-6eac-47f4-c4ff-70252c0cdd7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_df.shape= (15363, 6)\n",
      "test_df.shape= (16172, 6)\n"
     ]
    }
   ],
   "source": [
    "# Create validation and test partitions\n",
    "quora_df = pd.read_csv(\"$HOME/Datasets/QuoraQuestionPairs/quora_data.csv\")\n",
    "A_df, test_df = sklearn.model_selection.train_test_split(quora_df, test_size=0.05, random_state=123)\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(A_df, test_size=0.05)\n",
    "print('val_df.shape=',val_df.shape)\n",
    "print('test_df.shape=',test_df.shape)\n",
    "\n",
    "y_val = val_df[\"is_duplicate\"].values\n",
    "y_test = test_df[\"is_duplicate\"].values\n",
    "\n",
    "# cast to list taking care of nans:\n",
    "q1_val =  cast_list_as_strings(list(val_df[\"question1\"]))\n",
    "q2_val =  cast_list_as_strings(list(val_df[\"question2\"]))\n",
    "q1_test  =  cast_list_as_strings(list(test_df[\"question1\"]))\n",
    "q2_test  =  cast_list_as_strings(list(test_df[\"question2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11483598",
   "metadata": {
    "id": "11483598"
   },
   "source": [
    "### 9. Siamese LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99443585",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-05T07:38:45.442951Z",
     "iopub.status.busy": "2025-05-05T07:38:45.442621Z",
     "iopub.status.idle": "2025-05-05T07:39:10.347367Z",
     "shell.execute_reply": "2025-05-05T07:39:10.346563Z"
    },
    "id": "99443585",
    "outputId": "2c74edbe-ac2f-4c8a-f99f-0a2d6bef7911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from /Users/user/Documents/GitHub/nlp_deliv_1/models/model_lstm.pkl\n",
      "Model loaded from /Users/user/Documents/GitHub/nlp_deliv_1/models/tokenizer_tf.pkl\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step\n",
      "LSTM Accuracy: 0.8044768736087065\n"
     ]
    }
   ],
   "source": [
    "model_lstm = load_model(models_path, \"model_lstm.pkl\")\n",
    "# Prepare text for LSTM\n",
    "q1_train =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
    "q2_train =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
    "\n",
    "\n",
    "max_words = 20000\n",
    "max_len = 50\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = load_model(models_path, \"tokenizer_tf.pkl\")\n",
    "\n",
    "\n",
    "X_train_q1, X_train_q2 = texts_to_padded(q1_train, q2_train, tokenizer, max_len)\n",
    "X_val_q1,   X_val_q2   = texts_to_padded(q1_val, q2_val, tokenizer, max_len)\n",
    "X_test_q1,  X_test_q2  = texts_to_padded(q1_test, q2_test, tokenizer, max_len)\n",
    "# Evaluate LSTM\n",
    "y_pred_lstm = (model_lstm.predict([X_test_q1, X_test_q2]) > 0.6).astype(int)\n",
    "lstm_acc = accuracy_score(y_test, y_pred_lstm)\n",
    "\n",
    "print(f\"LSTM Accuracy: {lstm_acc}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
